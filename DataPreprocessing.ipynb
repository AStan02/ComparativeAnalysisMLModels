{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial preprocessing\n",
    "#Separates the data for Oslo airport, executed for each yearly csv\n",
    "df = df[df['IATA'].str.startswith('OSL')] \n",
    "df.to_csv('Avinor_AirTraffic2019_OSL.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial passenger data preprocessing\n",
    "#Execued for each yearly csv\n",
    "#Creates a new column in the df 'originaldestionation'\n",
    "#This new column is a combination of DepartureArrival (AvgangAnkomst) and InnlandUtland (DomesticInternational)\n",
    "#Data is grouped and passenger amount (termnalpassasjerer) is summed for each hourly index \n",
    "df = pd.read_csv('Avinor_AirTraffic2019_OSL.csv',encoding='latin-1')\n",
    "\n",
    "df['OriginDestination'] = df['AvgangAnkomst'] + '-' + df['InnlandUtland']\n",
    "df['KortDato'] = pd.to_datetime(df['KortDato'])\n",
    "\n",
    "hourly_passengers = df.groupby(['KortDato', 'Time', 'OriginDestination'])['Termnalpassasjerer'].sum().reset_index()\n",
    "hourly_passengers = hourly_passengers.groupby(['KortDato', 'Time'])['Termnalpassasjerer'].sum().reset_index()\n",
    "hourly_passengers = hourly_passengers.set_index(['KortDato', 'Time']).unstack('Time').stack('Time', dropna=False).reset_index()\n",
    "hourly_passengers = hourly_passengers.fillna(0)\n",
    "\n",
    "#Export only date hour and passenger amount\n",
    "hourly_passengers = hourly_passengers[['KortDato', 'Time', 'Termnalpassasjerer']]\n",
    "hourly_passengers.to_csv('Avinor_AirTraffic2019_OSL_FINAL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weather data preprocessing. Since the data is irregular, the data is processed using only the index nearest to a round hour\n",
    "#Air temperature and Mean wind speed are the two meteorological variables\n",
    "#Executed once for each yearly csv\n",
    "df_being_edited = '2009WeatherData.csv'\n",
    "df = pd.read_csv(df_being_edited)\n",
    "\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'], format='%d.%m.%Y %H:%M')\n",
    "df.set_index('Datetime', inplace=True)\n",
    "df_hourly = df.resample('H').nearest()\n",
    "\n",
    "cols = df_hourly[['Air temperature', 'Mean wind speed']]\n",
    "cols.to_csv(df_being_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds missing values in passenger data, dates and file name changed for each year\n",
    "#Also prints the amount of missing values found\n",
    "#Missing value rows are removed, with the corresponding rows in weather data also removed \n",
    "#Executed once for each yearly csv\n",
    "start_date = datetime(2019, 1, 1) \n",
    "end_date = datetime(2019, 12, 31, 23) \n",
    "all_datetimes = [start_date + timedelta(hours=i) for i in range((end_date - start_date).days * 24 + 24)]\n",
    "\n",
    "data = []\n",
    "with open('Avinor_AirTraffic2019_OSL_FINAL.csv', 'r') as f: \n",
    "    next(f) \n",
    "    for line in f:\n",
    "        date_str, hour_str, passengers = line.strip().split(',')\n",
    "        date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "        hour = int(hour_str)\n",
    "        data.append((date, hour))\n",
    "\n",
    "missing_datetimes = set(all_datetimes) - set(datetime.combine(date, datetime.min.time()) + timedelta(hours=hour) for date, hour in data)\n",
    "x = 0\n",
    "for dt in sorted(missing_datetimes):\n",
    "    x += 1\n",
    "    print(dt.strftime('%d/%m/%Y %H'))\n",
    "print(x)\n",
    "x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds missing dates in weather CSVs, dates and file name changed for each year\n",
    "#Also prints the amount of missing values found\n",
    "#Missing value rows are removed, with the corresponding rows in passenger data also removed \n",
    "#Executed once for each yearly csv\n",
    "start_date = datetime(2011, 1, 1) \n",
    "end_date = datetime(2011, 12, 31, 23) \n",
    "all_datetimes = [start_date + timedelta(hours=i) for i in range((end_date - start_date).days * 24 + 24)]\n",
    "\n",
    "data = []\n",
    "with open('2011WeatherData_Final.csv', 'r') as f: \n",
    "    next(f) \n",
    "    for line in f:\n",
    "        datetime_str = line.strip().split(',')[0]\n",
    "        datetime_obj = datetime.strptime(datetime_str, '%d/%m/%Y %H:%M')\n",
    "        data.append(datetime_obj)\n",
    "\n",
    "missing_datetimes = set(all_datetimes) - set(data)\n",
    "\n",
    "y = 0\n",
    "for dt in sorted(missing_datetimes):\n",
    "    y += 1\n",
    "    print(dt.strftime('%d/%m/%Y %H:%M:%S'))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is now ready to be merged, which is done using excel\n",
    "#This means that the weather data of air temperature and mean wind speed is merged chronologically with the passenger csv\n",
    "#The resulting yearly csv use the datetime, hourly and passenger amount features from the passenger data\n",
    "#Aswell as the air temperature and mean wind speed from the weather data\n",
    "#Preprocessing after this point and onward is done on these new merged yearly csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset being edited\n",
    "dataset_name = 'Air2019_Final.csv'\n",
    "data = pd.read_csv(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The datetime feature 'KortDato' is converted to a pandas datetime object\n",
    "#The day month and yearly components of the datetime are extracted as their own standalone features\n",
    "#Data is then just reindexed, with passengers column renamed for clarity\n",
    "data['KortDato'] = pd.to_datetime(data['KortDato'], format='%d/%m/%Y')\n",
    "data['Day'] = data['KortDato'].dt.day\n",
    "data['Month'] = data['KortDato'].dt.month\n",
    "data['Year'] = data['KortDato'].dt.year\n",
    "data = data.reindex(columns=['Time','Day','Month','Year','Termnalpassasjerer','Air temperature','Mean wind speed'])\n",
    "data = data.rename(columns={'Termnalpassasjerer':'Passengers'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following preprocessing is executed once for each yearly CSV\n",
    "#Checks for null, missing values, fixes wrong data types to numeric for weather variables\n",
    "#Confirms everything is in order and saves the csv\n",
    "#Check for null values\n",
    "mask = data[['Year','Passengers','Air temperature','Mean wind speed']].isnull()\n",
    "missing_data = data[mask.any(axis=1)]\n",
    "missing_data\n",
    "\n",
    "#check datatype for columns\n",
    "result = data.dtypes\n",
    "print(result)\n",
    "\n",
    "#Inspect unique values\n",
    "print('Air Temperature:')\n",
    "print(data['Air temperature'].unique())\n",
    "print('Mean wind speed:')\n",
    "print(data['Mean wind speed'].unique())\n",
    "\n",
    "#convert wrong datatypes to numeric\n",
    "data['Air temperature'] = pd.to_numeric(data['Air temperature'], errors='coerce')\n",
    "data['Mean wind speed'] = pd.to_numeric(data['Mean wind speed'], errors='coerce')\n",
    "\n",
    "print('Air Temperature:')\n",
    "print(data['Air temperature'].unique())\n",
    "print('Mean wind speed:')\n",
    "print(data['Mean wind speed'].unique())\n",
    "\n",
    "#drop null and save\n",
    "data = data.dropna()\n",
    "data.to_csv(dataset_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only when all the previous preprocessing is done for all individual yearly csv\n",
    "#Can then the files be merged into the complete csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a10117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merges all the finished yearly csvs, chronologically, into a complete csv\n",
    "files_to_merge = ['Air2009_Final.csv', 'Air2010_Final.csv', 'Air2011_Final.csv',\n",
    "                  'Air2012_Final.csv', 'Air2013_Final.csv', 'Air2014_Final.csv',\n",
    "                  'Air2015_Final.csv', 'Air2016_Final.csv', 'Air2017_Final.csv', \n",
    "                  'Air2018_Final.csv', 'Air2019_Final.csv']\n",
    "data_frames = []\n",
    "\n",
    "for file in files_to_merge:\n",
    "    df = pd.read_csv(file, index_col=None, header=0)\n",
    "    data_frames.append(df)\n",
    "\n",
    "merged_df = pd.concat(data_frames, axis=0, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv('Air2009-2019_Complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c96efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting the complete csv\n",
    "merged_df = pd.read_csv('Air2009-2019_Complete.csv', index_col=None, header=0)\n",
    "\n",
    "print('Number of rows:', len(merged_df))\n",
    "print('Data types of columns:')\n",
    "print(merged_df.dtypes)\n",
    "print(merged_df.head())\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting random samples \n",
    "#print x amount of samples of 10 consecutive rows\n",
    "sample_size = 10\n",
    "max_index = len(merged_df) - sample_size\n",
    "\n",
    "for i in range(20):\n",
    "    start_index = np.random.randint(0, max_index)\n",
    "    \n",
    "    end_index = start_index + sample_size\n",
    "    sample = merged_df.iloc[start_index:end_index]\n",
    "    print(f'Sample {i+1}:')\n",
    "    print(sample)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d5d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70901ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
