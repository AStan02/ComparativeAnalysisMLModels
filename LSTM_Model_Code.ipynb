{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7354e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.initializers import he_uniform\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in complete csv\n",
    "data = pd.read_csv('Air2009-2019_Complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1232c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data normalization, normalizes each features in the range of 0-1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46160625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57711\n",
      "19237\n",
      "19237\n"
     ]
    }
   ],
   "source": [
    "#Data split in a 60/20/20 ratio\n",
    "total_size = len(data)\n",
    "train_size = int(total_size * 0.6)\n",
    "val_size = int(total_size * 0.2)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_df = data[:train_size]\n",
    "val_df = data[train_size:train_size+val_size]\n",
    "test_df = data[train_size+val_size:]\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c257e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Hyperpamater definition and assignment\n",
    "#Additional model_name uses the parameter names for the tensorboard model name so progress can be inspected\n",
    "n_steps = 24\n",
    "n_features = 7\n",
    "n_epochs = 100\n",
    "n_batch = 128\n",
    "n_of_layers = 1\n",
    "n_of_neurons = '128'\n",
    "activation = 'tanh'\n",
    "lr = 0.001\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "optimizer_name = 'adam'\n",
    "final = 'True_Newest'\n",
    "\n",
    "model_name = f\"S={n_steps}L={n_of_layers}N={n_of_neurons}A={activation}O={optimizer_name}LR={lr}B={n_batch}F={final}\"\n",
    "tensorboard = TensorBoard(log_dir=f'logs/fit/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544e6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function creates sequences from the data based on the amount of n_steps\n",
    "def lstm_sequence(data, n_steps):\n",
    "    x, y = [], []\n",
    "    for i in range(n_steps, len(data)):\n",
    "        x.append(data[i-n_steps:i, :])\n",
    "        y.append(data[i, 4]) #i,passenger column\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05a75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data sequences creates for each train, test dataset for training, validation and testing\n",
    "x_train, y_train = lstm_sequence(train_df, n_steps)\n",
    "x_val, y_val = lstm_sequence(val_df, n_steps)\n",
    "x_test, y_test = lstm_sequence(test_df, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa5ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation=activation,\n",
    "                    kernel_initializer=glorot_uniform(seed=42),\n",
    "                    input_shape=(n_steps, n_features)))\n",
    "model.add(Dropout(rate=0.2,seed=42))\n",
    "model.add(Dense(units=1, activation='linear', \n",
    "                         kernel_initializer=glorot_uniform(seed=42)))\n",
    "\n",
    "model.compile(optimizer=optimizer,loss='mean_squared_error',metrics=['mean_absolute_error'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f7cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "451/451 [==============================] - 7s 10ms/step - loss: 0.0128 - mean_absolute_error: 0.0850 - val_loss: 0.0080 - val_mean_absolute_error: 0.0671\n",
      "Epoch 2/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0058 - mean_absolute_error: 0.0583 - val_loss: 0.0063 - val_mean_absolute_error: 0.0572\n",
      "Epoch 3/100\n",
      "451/451 [==============================] - 5s 10ms/step - loss: 0.0049 - mean_absolute_error: 0.0529 - val_loss: 0.0054 - val_mean_absolute_error: 0.0548\n",
      "Epoch 4/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0046 - mean_absolute_error: 0.0501 - val_loss: 0.0053 - val_mean_absolute_error: 0.0543\n",
      "Epoch 5/100\n",
      "451/451 [==============================] - 4s 9ms/step - loss: 0.0043 - mean_absolute_error: 0.0484 - val_loss: 0.0050 - val_mean_absolute_error: 0.0516\n",
      "Epoch 6/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0465 - val_loss: 0.0047 - val_mean_absolute_error: 0.0502\n",
      "Epoch 7/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0038 - mean_absolute_error: 0.0450 - val_loss: 0.0048 - val_mean_absolute_error: 0.0494\n",
      "Epoch 8/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0437 - val_loss: 0.0044 - val_mean_absolute_error: 0.0480\n",
      "Epoch 9/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0427 - val_loss: 0.0043 - val_mean_absolute_error: 0.0478\n",
      "Epoch 10/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0034 - mean_absolute_error: 0.0422 - val_loss: 0.0044 - val_mean_absolute_error: 0.0481\n",
      "Epoch 11/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0033 - mean_absolute_error: 0.0413 - val_loss: 0.0041 - val_mean_absolute_error: 0.0461\n",
      "Epoch 12/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0032 - mean_absolute_error: 0.0409 - val_loss: 0.0042 - val_mean_absolute_error: 0.0473\n",
      "Epoch 13/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0031 - mean_absolute_error: 0.0404 - val_loss: 0.0040 - val_mean_absolute_error: 0.0467\n",
      "Epoch 14/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0031 - mean_absolute_error: 0.0401 - val_loss: 0.0037 - val_mean_absolute_error: 0.0437\n",
      "Epoch 15/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0397 - val_loss: 0.0039 - val_mean_absolute_error: 0.0461\n",
      "Epoch 16/100\n",
      "451/451 [==============================] - 4s 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0393 - val_loss: 0.0038 - val_mean_absolute_error: 0.0436\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "source": [
    "#The model is fit the training data, with the parameters for epochs, batch declared\n",
    "#Validation data is used to cross-validate aswell as by the early_stop callback\n",
    "#Tensorboard callback also used for later inspection\n",
    "train_model = model.fit(x_train, y_train, \n",
    "           epochs=n_epochs, \n",
    "           batch_size=n_batch,\n",
    "           validation_data=(x_val, y_val),\n",
    "           callbacks=[early_stop,tensorboard],\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "822afc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00268 MSE (0.05178 RMSE)\n",
      "Validation Score: 0.00377 MSE (0.06141 RMSE)\n",
      "Test Score: 0.00445 MSE (0.06667 RMSE)\n"
     ]
    }
   ],
   "source": [
    "#LSTM Evaluation using the model.evaluate() method on all 3 datasets\n",
    "#The corresponding MSE and RMSE printed for each evaluation\n",
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "val_score = model.evaluate(x_val, y_val, verbose=0)\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Train Score: {:.5f} MSE ({:.5f} RMSE)'.format(train_score[0], np.sqrt(train_score[0])))\n",
    "print('Validation Score: {:.5f} MSE ({:.5f} RMSE)'.format(val_score[0], np.sqrt(val_score[0])))\n",
    "print('Test Score: {:.5f} MSE ({:.5f} RMSE)'.format(test_score[0], np.sqrt(test_score[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f6b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction code\n",
    "y_pred = model.predict(x_test)\n",
    "#Some reshaping necessary, involving making \"dummy\" numpy arrays filled with 0s, \n",
    "#Then fitting data into the dummy arrays in order to avoid shape errors when making the predictions\n",
    "y_test_dummy = np.zeros((y_test.shape[0], 7))\n",
    "y_pred_dummy = np.zeros((y_pred.shape[0], 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287f779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using ravel to replace each empty value in index 4 (column 5) with the y_test and y_pred sets\n",
    "y_test_dummy[:, 4] = y_test.ravel()\n",
    "y_pred_dummy[:, 4] = y_pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1bfff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the remaining values from each feature in the x_test, this means the features up until index 4\n",
    "#And all features after 4, the resulting y_test_dummy and y_pred_dummmy are ready for inverse transform\n",
    "y_test_dummy[:, :4] = x_test[:, -1, :4]\n",
    "y_pred_dummy[:, :4] = x_test[:, -1, :4]\n",
    "y_test_dummy[:, 5:] = x_test[:, -1, 5:]\n",
    "y_pred_dummy[:, 5:] = x_test[:, -1, 5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db2284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse transform just removes the normalization to return the values in index 4 (passengers) to the realistic scale in order to derive insight from the predictions\n",
    "y_test_inv = np.round(scaler.inverse_transform(y_test_dummy)[:, 4])\n",
    "y_pred_inv = np.round(scaler.inverse_transform(y_pred_dummy)[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68b3aae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Actual: 1541.0, Predicted: 1310.0\n",
      "Index: 1, Actual: 2097.0, Predicted: 1743.0\n",
      "Index: 2, Actual: 2398.0, Predicted: 2273.0\n",
      "Index: 3, Actual: 2540.0, Predicted: 2889.0\n",
      "Index: 4, Actual: 4294.0, Predicted: 3405.0\n",
      "Index: 5, Actual: 5761.0, Predicted: 4508.0\n",
      "Index: 6, Actual: 5273.0, Predicted: 5883.0\n",
      "Index: 7, Actual: 6506.0, Predicted: 6495.0\n",
      "Index: 8, Actual: 6654.0, Predicted: 6960.0\n",
      "Index: 9, Actual: 6876.0, Predicted: 7280.0\n",
      "Index: 10, Actual: 7036.0, Predicted: 7388.0\n",
      "Index: 11, Actual: 7919.0, Predicted: 7206.0\n",
      "Index: 12, Actual: 7723.0, Predicted: 6866.0\n",
      "Index: 13, Actual: 6387.0, Predicted: 6264.0\n",
      "Index: 14, Actual: 5894.0, Predicted: 5286.0\n",
      "Index: 15, Actual: 4796.0, Predicted: 4393.0\n",
      "Index: 16, Actual: 2184.0, Predicted: 2331.0\n",
      "Index: 17, Actual: 499.0, Predicted: 513.0\n",
      "Index: 18, Actual: 181.0, Predicted: 115.0\n",
      "Index: 19, Actual: 0.0, Predicted: 70.0\n",
      "Index: 20, Actual: 0.0, Predicted: 79.0\n",
      "Index: 21, Actual: 0.0, Predicted: 13.0\n",
      "Index: 22, Actual: 99.0, Predicted: 11.0\n",
      "Index: 23, Actual: 1272.0, Predicted: 1075.0\n",
      "Index: 24, Actual: 5304.0, Predicted: 4100.0\n",
      "Index: 25, Actual: 7067.0, Predicted: 7774.0\n",
      "Index: 26, Actual: 5458.0, Predicted: 5513.0\n",
      "Index: 27, Actual: 4727.0, Predicted: 4663.0\n",
      "Index: 28, Actual: 4122.0, Predicted: 4252.0\n",
      "Index: 29, Actual: 4826.0, Predicted: 3696.0\n",
      "Index: 30, Actual: 4337.0, Predicted: 4456.0\n",
      "Index: 31, Actual: 5399.0, Predicted: 4761.0\n",
      "Index: 32, Actual: 5547.0, Predicted: 5495.0\n",
      "Index: 33, Actual: 5517.0, Predicted: 6436.0\n",
      "Index: 34, Actual: 5506.0, Predicted: 6681.0\n",
      "Index: 35, Actual: 5210.0, Predicted: 5596.0\n",
      "Index: 36, Actual: 5218.0, Predicted: 4710.0\n",
      "Index: 37, Actual: 3667.0, Predicted: 3585.0\n",
      "Index: 38, Actual: 3236.0, Predicted: 2568.0\n",
      "Index: 39, Actual: 1881.0, Predicted: 1906.0\n",
      "Index: 40, Actual: 943.0, Predicted: 1181.0\n",
      "Index: 41, Actual: 583.0, Predicted: 312.0\n",
      "Index: 42, Actual: 0.0, Predicted: 86.0\n",
      "Index: 43, Actual: 0.0, Predicted: 28.0\n",
      "Index: 44, Actual: 0.0, Predicted: 3.0\n",
      "Index: 45, Actual: 0.0, Predicted: -110.0\n",
      "Index: 46, Actual: 0.0, Predicted: -29.0\n",
      "Index: 47, Actual: 1333.0, Predicted: 1057.0\n"
     ]
    }
   ],
   "source": [
    "#Print an x amount of actual vs predicted values\n",
    "index = 0\n",
    "for i in range(48):\n",
    "    print(f\"Index: {index}, Actual: {y_test_inv[i]}, Predicted: {y_pred_inv[i]}\")\n",
    "    index+=1\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac8ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b373861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45809e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70622cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b53d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
